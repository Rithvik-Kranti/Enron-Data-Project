{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is used in \"feature set compression.\"\n",
    "\n",
    "PCA specializes on shifts and rotation for the coordinate system. \n",
    "\n",
    "By translational and rotation, PCA finds a new coordinate system and moves the centre of coordinate system from old to new.\n",
    "It moves x axis into the principal axis of variation where you see the most variation relative to all data points and y axis orthogonal to the new x where there is less variation. It tells you how important the two axes are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal vectors found by PCA:**\n",
    "1. PC Vecotr 1: (delta x) and (delta y) for x axis.\n",
    "2. PC Vecotr 2: (delta x) and (delta y) for y axis.\n",
    "\n",
    "Each of for x axis and y axis : add to 1 : vector length = square root of sum of squares.\n",
    "\n",
    "The two vectors are orthogonal i.e ( delta x1 multiply delta y1 ) +  ( delta x2 multiply delta y2 ) = 0 .\n",
    "\n",
    "PCA finds the centre of data and the principal axis of variation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA also returns an importance value : a spread value for these axes.\n",
    "\n",
    "The spread value is large for first axis of variation and much smaller for second axis of variation. This number happens to be a **eigen value**. It comes out of an eigenvalue decomposition that's implemented by PCA. It gives you the importance vector , ** how important to take each axis when you look at the data .** \n",
    "\n",
    "So when you run the code with PCA, you will find : \n",
    "\n",
    "**1.** The new origin.\n",
    "\n",
    "**2.** Different vectors .\n",
    "\n",
    "**3.** An importance value , that measures the amount of spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> **PCA and Linear Regression**\n",
    "\n",
    "1. Its impossible to build a regression that goes vertically up ((i.e constant x with varying y )), because you can't divide the data set as a function of y = f(x). \n",
    "\n",
    "2. Regression treats the variables(one is input, one is output) asymmetrically.\n",
    "\n",
    "3. In PCA, all we get is vectors .\n",
    "So we can make a coordinate system where the x axis falls vertically with the alined vertically up data , and a perpendicular y-axis .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have data like a circle, there could still be a main axis and a secondary axis with PCA.\n",
    "\n",
    "The both Eigen values can be of same magnitude and we won't gain much by running PCA. So, not always the major axis would be dominating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ** Measurable v/s Latent Features**\n",
    "\n",
    "*Measurable* : \n",
    "1. Sq. footage.\n",
    "2. No. of rooms.\n",
    "3. School ranking.\n",
    "4. Neighbourhood safety.\n",
    "\n",
    "*Latent* : Variables that can't be measured directly, but that drive the phenomenon behind the scenes.\n",
    "1. Size.(( Sq. footage & No. of rooms ))\n",
    "2. Neighbourhood. (( School ranking &  Neighbourhood safety ))\n",
    "\n",
    "So the 4 Measurable features have been reduced to 2 Latent features.\n",
    "\n",
    "** How best to condense our 'N' Measurable features to 2 Latent features obtained so that we don't lose essential info ? **\n",
    "\n",
    "** Which feature selection tool would be most suitable for this ? **  Select KBest or Select Percentile. ?\n",
    "\n",
    "**A**:  Select KBest because you know how many you want to get out..Here you want to get 2 from the lot available . So it will throw away all except the 2 that are most powerful.\n",
    "\n",
    "Select Percentile is not good here because you don't already know exactly how many features you have ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Many features are present. But let's say we hypothesize that there are a small number of features which actually drive the whole phenomenon , patterns.\n",
    "\n",
    "2. Try making a composite feature that more directly probes this phenomenon .\n",
    "\n",
    "These composite features , are called , **Principal Components** .\n",
    "\n",
    "These are talked in terms of **Dimensionality reduction** .\n",
    "How you can use PCA to bring down the dimensionality of your features to turn a bunch of features into a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> **  Determining the Principal Component ** : \n",
    "\n",
    "**Variance** :  \n",
    "1. The willingness/flexibility of an algorithm to learn.\n",
    "2. Technical term in statistics : roughly the spread of a data distribution ( similar to standard deviation)\n",
    "\n",
    "Principal Component of a dataset is the direction that has the largest variance in the data because it will retain the maximum information from orginal data when we transfer the 2-D data to 1-D data.\n",
    "\n",
    "((((In regression , you try to make a prediction . In PCA, you try to find the direction of maximum variance)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** Maximal Variance and Information Loss: **\n",
    "\n",
    "Amount of info lost = distance between a given point in 2-D and the new spot on the line\n",
    "\n",
    "Amount of info lost is proportional to that distance for that point.\n",
    "\n",
    "**Information Loss**  = sum of all the distances (as explained above)  \n",
    "\n",
    "When we do the projection on to the direction of maximal variance,and only onto that direction , we'll be able to minimizing the information loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "--systematized way to transform input features into principal components\n",
    "-- use principal componentts as new features\n",
    "--pc’s are directions in data that maximize variance ( minimize information loss ) when you project/ compress down onto them. \n",
    "--also rank the prinicpal comps---more varaince of data along pc , higher that pc is ranked.\n",
    "   one that has most variance, will be first princ comp.\n",
    "--pcs are all perpendicular to each other..—so 2nd pcomp is guaranteed to not overlap the 1st—so you can treat them as independ features in the sense.\n",
    "--max number of pcomponents=no. of input features you had in the dataset.\n",
    "usually, you will use only the first handful of pcomps. But can use all.but you wont gain anything..its just you are reperesenting all features in diff way if u use all\n",
    "\n",
    "When to use pca:\n",
    "1)\tTo have access to latent features driving the patterns in the data((like big shots in enron)) –in other words u want to know the size of the first princi comp.\n",
    "2)\tDimensionality reduction helps-\n",
    "a.\tVisualize high dimensional data\n",
    "b.\tReduce noise \n",
    "c.\tMake other algorithms((reg,class)) work better because fewer inputs((eigen faces)) i.e pca as pre-processing before u use other algo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
