{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is used in \"feature set compression.\"\n",
    "\n",
    "PCA specializes on shifts and rotation for the coordinate system. \n",
    "\n",
    "By translational and rotation, PCA finds a new coordinate system and moves the centre of coordinate system from old to new.\n",
    "It moves x axis into the principal axis of variation where you see the most variation relative to all data points and y axis orthogonal to the new x where there is less variation. It tells you how important the two axes are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal vectors found by PCA:**\n",
    "1. PC Vecotr 1: (delta x) and (delta y) for x axis.\n",
    "2. PC Vecotr 2: (delta x) and (delta y) for y axis.\n",
    "\n",
    "Each of for x axis and y axis : add to 1 : vector length = square root of sum of squares.\n",
    "\n",
    "The two vectors are orthogonal i.e ( delta x1 multiply delta y1 ) +  ( delta x2 multiply delta y2 ) = 0 .\n",
    "\n",
    "PCA finds the centre of data and the principal axis of variation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA also returns an importance value : a spread value for these axes.\n",
    "\n",
    "The spread value is large for first axis of variation and much smaller for second axis of variation. This number happens to be a **eigen value**. It comes out of an eigenvalue decomposition that's implemented by PCA. It gives you the importance vector , ** how important to take each axis when you look at the data .** \n",
    "\n",
    "So when you run the code with PCA, you will find : \n",
    "\n",
    "**1.** The new origin.\n",
    "\n",
    "**2.** Different vectors .\n",
    "\n",
    "**3.** An importance value , that measures the amount of spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> **PCA and Linear Regression**\n",
    "\n",
    "1. Its impossible to build a regression that goes vertically up ((i.e constant x with varying y )), because you can't divide the data set as a function of y = f(x). \n",
    "\n",
    "2. Regression treats the variables(one is input, one is output) asymmetrically.\n",
    "\n",
    "3. In PCA, all we get is vectors .\n",
    "So we can make a coordinate system where the x axis falls vertically with the alined vertically up data , and a perpendicular y-axis .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have data like a circle, there could still be a main axis and a secondary axis with PCA.\n",
    "\n",
    "The both Eigen values can be of same magnitude and we won't gain much by running PCA. So, not always the major axis would be dominating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ** Measurable v/s Latent Features**\n",
    "\n",
    "*Measurable* : \n",
    "1. Sq. footage.\n",
    "2. No. of rooms.\n",
    "3. School ranking.\n",
    "4. Neighbourhood safety.\n",
    "\n",
    "*Latent* : Variables that can't be measured directly, but that drive the phenomenon behind the scenes.\n",
    "1. Size.(( Sq. footage & No. of rooms ))\n",
    "2. Neighbourhood. (( School ranking &  Neighbourhood safety ))\n",
    "\n",
    "So the 4 Measurable features have been reduced to 2 Latent features.\n",
    "\n",
    "** How best to condense our 'N' Measurable features to 2 Latent features obtained so that we don't lose essential info ? **\n",
    "\n",
    "** Which feature selection tool would be most suitable for this ? **  Select KBest or Select Percentile. ?\n",
    "\n",
    "**A**:  Select KBest because you know how many you want to get out..Here you want to get 2 from the lot available . So it will throw away all except the 2 that are most powerful.\n",
    "\n",
    "Select Percentile is not good here because you don't already know exactly how many features you have ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Many features are present. But let's say we hypothesize that there are a small number of features which actually drive the whole phenomenon , patterns.\n",
    "\n",
    "2. Try making a composite feature that more directly probes this phenomenon .\n",
    "\n",
    "These composite features , are called , **Principal Components** .\n",
    "\n",
    "These are talked in terms of **Dimensionality reduction** .\n",
    "How you can use PCA to bring down the dimensionality of your features to turn a bunch of features into a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> **  Determining the Principal Component ** : \n",
    "\n",
    "**Variance** :  \n",
    "1. The willingness/flexibility of an algorithm to learn.\n",
    "2. Technical term in statistics : roughly the spread of a data distribution ( similar to standard deviation)\n",
    "\n",
    "Principal Component of a dataset is the direction that has the largest variance in the data because it will retain the maximum information from orginal data when we transfer the 2-D data to 1-D data.\n",
    "\n",
    "((((In regression , you try to make a prediction . In PCA, you try to find the direction of maximum variance)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** Maximal Variance and Information Loss: **\n",
    "\n",
    "Amount of info lost = distance between a given point in 2-D and the new spot on the line\n",
    "\n",
    "Amount of info lost is proportional to that distance for that point.\n",
    "\n",
    "**Information Loss**  = sum of all the distances (as explained above)  \n",
    "\n",
    "When we do the projection on to the direction of maximal variance, and only onto that direction , we'll be able to minimizing the information loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** You can put all features available into PCA together, and it automatically, gives you : First Principal Component and Second Principal Component. and you need to understand the components actual names i.e what is the main driving phenomenon. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Maximum number of PCA's allowed by sklearn.\n",
    "\n",
    "**A.** It's minimum of the number of training points and the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **PCA review **:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Systematized way to transform input features into principal components.\n",
    "2. Those principal componentts are available as new features to use instead of the original input features.\n",
    "3. Pcâ€™s are directions in data that maximize variance ( minimize information loss ) when you project/ compress down onto those PCA. \n",
    "4. Also rank the prinicpal comps---more varaince of data along pc , higher that pc is ranked.\n",
    "   One that has most variance(most info), will be first princ comp and so on.\n",
    "5. PC's are all perpendicular to each other, so 2nd PC is guaranteed to not overlap the 1st, 3rd wont overlap 2nd, and so on, so you can treat them as independent features in the sense.\n",
    "6. Max number of PC's=no. of input features you had in the dataset.Usually, you will use only the first handful of PC's. But can use all.but you wont gain anything..ts just you are reperesenting all features in diff way if u use all.\n",
    "\n",
    "\n",
    "**When to use PCA:**\n",
    "\n",
    "1. To have access to latent features driving the patterns in the data. In other words u want to know the size of the first princi comp, to try to figure out whether there is latent feature.((like can you measure who big shots in enron are.))\n",
    "2. Dimensionality reduction helps :\n",
    "\n",
    "    a.\tVisualize high dimensional data.((if you have many features, make them into PC's and make scatterplots)).\n",
    "    \n",
    "    b.\tReduce noise.((1st and 2nd PCA will mostly capture the most of the data, and the next smaller PCA's are just noise)).\n",
    "    \n",
    "    c.\tMake other algorithms((reg,class)) work better because fewer inputs((eigen faces)) i.e PCA as pre-processing before you use other algo. With high dimensions, the algorithm can be high variance, it might fit the noise to data and can run slow.\n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **PCA in sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca=PCA(n_components=2)\n",
    "\n",
    "pca.fit(data)\n",
    "\n",
    "# attribues: eigen values and components.\n",
    "pca.explained_variance_ratio_   # where the eigen values live , how much variation the 1st pca has, how much 2nd pca so on.\n",
    "\n",
    "first_pc= pca.components_[0]\n",
    "second_pc= pca.components_[1]\n",
    "\n",
    "\n",
    "#### facial recog with PCA in sklearn\n",
    "\n",
    "PCA is also called **EigenFaces** when applied to facial recognition.\n",
    "\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "pca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w)) ## PC's of the face data.\n",
    "\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "\n",
    "## then create SVM ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perpedicularity observing with PCA:**\n",
    "\n",
    "The projection step of PCA can be easiest to understand when you subtract out the mean shift of the new principal components, so the new and old dimensions have the same mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** PCA for facial recognition: **\n",
    "\n",
    "1. Pictures of faces have high input dimensionality (many pixels)\n",
    "2. Faces have general patterns that could be captured in smaller number of dimensions.((eyes a bit together, chin types.etc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multiclass classification problem like Facial recognition (more than 2 labels to apply), accuracy is a less-intuitive metric than in the 2-class case. Instead, a popular metric is the **F1 score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: As you add more principal components as features for training your classifier, do you expect it to get better or worse performance?\n",
    "\n",
    "**A**: Ideally, we hope that adding more components will give us more signal information to improve the classifier performance.\n",
    "\n",
    "**Q**: If you see a higher F1 score, does it mean the classifier is doing better, or worse?\n",
    "\n",
    "**A**: Yes, higher means better!\n",
    "\n",
    "**Q**: Do you see any evidence of overfitting when using a large number of PCs? Does the dimensionality reduction of PCA seem to be helping your performance here?\n",
    "\n",
    "**A**: Yes, the F1 score(performance) starts to drop with many PC's here ((in facial recog prob)).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Selecting the number of PC's:  ** \n",
    "\n",
    "**Q** : What's a good way to figure out how many PC's to use.\n",
    "\n",
    "**A** : Train on different number of PC's and see how accuracy responds - cut off when it becomes apparent that adding more PC's doesn't buy you much more discrimination.\n",
    "\n",
    "\n",
    "If you do feature selection before putting them into PCA, you are already throwing out some info which PCA could find useful. So you would be losing a lot info. PCA might be able to put to use if not thrown already with feature selection.\n",
    "\n",
    "It's fine to do feature selection after making PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
