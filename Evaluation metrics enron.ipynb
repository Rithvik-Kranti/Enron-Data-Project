{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics :\n",
    "\n",
    "Say football game.\n",
    "\n",
    "Metric 1 : how many games won.\n",
    "\n",
    "Metric 2 : how many goals score.\n",
    "\n",
    "Metric 3 : how long does the game last. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy:** \n",
    "\n",
    "1. No. of data points labeled correctly / all data points. \n",
    "2. Accuracy problem when skewed classes : Not an ideal metric in cases where you have very few examples of one of the classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall: ////Sensitivity **\n",
    "\n",
    "If we are looking at performance on a specific class, then we are looking at recall.\n",
    "\n",
    "1. How many +ve items were recalled from dataset.\n",
    "2. TP/ (TP+FN).\n",
    "3. Recall is a measure of completeness or quantity.\n",
    "4. Associated with type 2 error.(FN)\n",
    "5. High recall means that an algorithm returned most of the relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision: **\n",
    "\n",
    "1. Out of all items labelled as +ve, how many truly belong to the +ve class.\n",
    "2. TP/ (TP+FP).\n",
    "2. Precision can be seen as a measure of exactness or quality.\n",
    "4. Associated with type 1 error.(FP)\n",
    "5. High precision means that an algorithm returned substantially more relevant results than irrelevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "precision_score(labels_test,clf.predict(features_test)\n",
    "                \n",
    "recall_score(labels_test,clf.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.666666666667\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "#  Let’s use the convention that “1” signifies a positive result, and “0” a negative. \n",
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1]\n",
    "true_labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "\n",
    "TP = 6\n",
    "TN = 9\n",
    "FP = 3\n",
    "FN = 2 \n",
    "\n",
    "Precision = 6/(6+3) \n",
    "Recall = 6/ (6+2)\n",
    "\n",
    "print (precision_score(true_labels,predictions))\n",
    "print (recall_score(true_labels,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> **Making sense of metrics: **\n",
    "\n",
    "1. My true positive rate is high, which means that when a POI is present in the test data, I am good at flagging him or her.\n",
    "2. My identifier doesn’t have great precision, but it does have good recall. That means that, nearly every time a POI shows up in my test set, I am able to identify him or her. The cost of this is that I sometimes get some false positives, where non-POIs get flagged.\n",
    "3. My identifier doesn’t have great recall, but it does have good precision. That means that whenever a POI gets flagged in my test set, I know with a lot of confidence that it’s very likely to be a real POI and not a false alarm. On the other hand, the price I pay for this is that I sometimes miss real POIs, since I’m effectively reluctant to pull the trigger on edge case.\n",
    "4. My identifier has a really great F1 score.This is the best of both worlds. Both my false positive and false negative rates are low, which means that I can identify POI’s reliably and accurately. If my identifier finds a POI then the person is almost certainly a POI, and if the identifier does not flag someone, then they are almost certainly not a POI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Precision imp. over Recall: **\n",
    "\n",
    "Precision is more important than recall when you would like to have less False Positives in trade off to have more False Negatives. Meaning, getting a False Positive is very costly, and a False Negative is not as much.\n",
    "\n",
    "In a zombie apocalypse, of course you would try to accept as many as healthy people you can into your safe zone, but you really dont want to mistakenly pass a zombie into the safe zone. So if your method causes some of the healthy people mistakenly not to get into the safe zone, then so be it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
