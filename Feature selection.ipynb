{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select best features.\n",
    "2. Add new features.\n",
    "\n",
    "        step 1. Use your intuition.\n",
    "        step 2. Code up the new feature.\n",
    "        step 3. Visualize.\n",
    "        step 4. Repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Getting rid of few features :  **\n",
    "\n",
    "Reasons:\n",
    "\n",
    "1. It's noisy .\n",
    "2. It cause overfitting.\n",
    "3. It is strongly related ( highly correlated ) with a feautre already present.\n",
    "4. Additional features slows down training/testing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features is like quantity.\n",
    "Information is quality.\n",
    "\n",
    "Not all features give relevant information.\n",
    "\n",
    "If there are lots of features,  that's quantity of data. \n",
    "\n",
    "One should develop a model with bare minimum number of features that it takes to give as much information as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Feature selection tools: ** \n",
    "\n",
    "There are two big univariate feature selection tools in sklearn: ** SelectPercentile **  and ** SelectKBest.** The difference is : SelectPercentile selects the X% of features that are most powerful (where X is a parameter) and SelectKBest selects the K features that are most powerful (where K is a parameter).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Bias-Variance dilemma: **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Bias  : Pays little attention to data. And is oversimplified. It does same thing over and over again regardless of what the data might be trying to tell it do.\n",
    "\n",
    "High Variance: Pays too much attention to data . Does not generalize well to new situations . It's basically memorizing the training examples . Its overfitting the data. \n",
    "\n",
    "High bias : Tend to have high error on training set. \n",
    "example on Regression: low r squared value , large SSE (sum of squared error).\n",
    "\n",
    "High variance : Tend to have high error on testing set.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using very less features : high bias.---oversimplified situation--underfitting.\n",
    "\n",
    "Using lots of features : high variance---overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization  :\n",
    "\n",
    "It is an automatic form of feature selection that some algorithms can do on their own that they can trade off between the precision, the goodness of fit,the very low errors, and the complexity of fitting lots of features.\n",
    "\n",
    "Regularization is a method for automatically penalizing extra features.\n",
    "\n",
    "Example : ** Lasso Regression ** \n",
    "\n",
    "Minimize the SSE + Minimize the number of features.(( penalty parameter * coefficients of regression ))\n",
    "\n",
    "Lasso regression can set coefficient of a feature to very small value, almost to zero, for features that don't help the regression results enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lasso in sklearn: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model.Lasso # importing the model\n",
    "features, labels  = data()        # getting the features and labels with data available\n",
    "regression = Lasso ()             # calling the regression\n",
    "regression.fit (features, labels) # features and targets to the regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Predicting new label with the above code for a new point : **\n",
    "\n",
    "Lets say the new point has the features 2 and 4 : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing to the function the features of the new point to make the prediction \n",
    "\n",
    "regression.predict ([2,4])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Finding coefficients which are small, if any, which is like discarding those features: **  \n",
    "\n",
    "Accessing features which are important in the regression results : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"regression.coef_\") # prints the coefficients of the regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Interpreting the coefficients : **\n",
    "\n",
    "The print statement above prints ,let's say a list of coefficients on different features with two elements say : [0.7, 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there is only one feature that matters the most.\n",
    "\n",
    "By the output of [0.7, 0.0] , there are two available with their strengths.\n",
    "\n",
    "The first one coefficient is 0.7.\n",
    "\n",
    "The second one coefficient is 0.0, which means that it is effectively not being used in the regression.\n",
    "Therefore, the second feature can be disregarded in this particular regression example. and the main information is coming from the first feature.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
