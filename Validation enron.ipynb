{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** : Why train-test split?\n",
    "\n",
    "**A** : \n",
    "1. Gives estimate of performance on an independent dataset.\n",
    "2.  Serves as check for overfitting.\n",
    "\n",
    "Drawback : \n",
    "If you hold out 10 % or 20 % of data for testing , then that data is not going to be available when training.\n",
    "\n",
    "Testing data is only something used as a tool for validating the steps we took when we were training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "\n",
    "from sklearn import cross_validation # for version 0.17\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # For version 0.18\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(\n",
    "                                                features, labels,test_size=0.4,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where to use training , where to use testing: **\n",
    "\n",
    "*example:*\n",
    "\n",
    "1. **Training**:\n",
    "    1. pca.fit (training _features)\n",
    "    2. pca.transform (training _features)\n",
    "    3. svc.fit(training_features)\n",
    "2. **Testing**:\n",
    "    1. pca.transform(testing_features) ## representing test data with PC's found on training data.\n",
    "    2. svc.predict(testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold( len(authors), 2 ) \n",
    "# 2 arguments : one is the number of items in the total dataset ,2nd argu is how many folds you want to look at.\n",
    "\n",
    "# simple way to randomize the events in sklearn k-fold CV: set the shuffle flag to true.\n",
    "cv = KFold( len(authors), 2, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "< your clf.fit() line of code >\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **KFold() & StratifiedKFold():**\n",
    "\n",
    "If our original data comes in some sort of sorted fashion, then we will want to first shuffle the order of the data points before splitting them up into folds, or otherwise randomly assign data points to each fold. If we want to do this using KFold(), then we can add the \"shuffle = True\" parameter when setting up the cross-validation object.\n",
    "\n",
    "If we have concerns about class imbalance, then we can use the StratifiedKFold() class instead. Where KFold() assigns points to folds without attention to output class, StratifiedKFold() assigns data points to folds so that each fold has approximately the same number of data points of each output class. This is most useful for when we have imbalanced numbers of data points in your outcome classes (e.g. one is rare compared to the others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearchCV \n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kfold CV uses :**\n",
    "\n",
    "1. For validating the performance of your algorithm.\n",
    "2. CV picks out parameter tune that's going to be best.---GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
